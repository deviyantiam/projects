{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The project is to identify topic of news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install PySastrawi\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer#, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "prepare the dataset by gathering and checking if theres any missing value and remove them\n",
    "## Data Gathering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataset = (10000, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>article_topic</th>\n",
       "      <th>article_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>93205794</td>\n",
       "      <td>Internasional</td>\n",
       "      <td>Kepolisian Inggris tengah memburu pelaku yang...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>93186698</td>\n",
       "      <td>Ekonomi</td>\n",
       "      <td>Seluruh layanan transaksi di jalan tol akan m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>93191463</td>\n",
       "      <td>Teknologi</td>\n",
       "      <td>\\nHari ini, Rabu (23/8), ternyata menjadi har...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>93219292</td>\n",
       "      <td>Ekonomi</td>\n",
       "      <td>Saat ini Indonesia hanya memiliki cadangan ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>343106</td>\n",
       "      <td>Hiburan</td>\n",
       "      <td>Hari ini, Selasa (1/8), pedangdut Ridho Rhoma...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id  article_topic  \\\n",
       "0    93205794  Internasional   \n",
       "1    93186698        Ekonomi   \n",
       "2    93191463      Teknologi   \n",
       "3    93219292        Ekonomi   \n",
       "4      343106        Hiburan   \n",
       "\n",
       "                                     article_content  \n",
       "0   Kepolisian Inggris tengah memburu pelaku yang...  \n",
       "1   Seluruh layanan transaksi di jalan tol akan m...  \n",
       "2   \\nHari ini, Rabu (23/8), ternyata menjadi har...  \n",
       "3   Saat ini Indonesia hanya memiliki cadangan ba...  \n",
       "4   Hari ini, Selasa (1/8), pedangdut Ridho Rhoma...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## collecting data\n",
    "data_csv=pd.read_csv('data.csv')\n",
    "print('Shape of dataset =', data_csv.shape)\n",
    "data_csv.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "article_id          0\n",
       "article_topic       0\n",
       "article_content    36\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## check missing value\n",
    "data_csv.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>article_topic</th>\n",
       "      <th>article_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>93210288</td>\n",
       "      <td>Teknologi</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>674</th>\n",
       "      <td>93185319</td>\n",
       "      <td>Hiburan</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>817</th>\n",
       "      <td>93189481</td>\n",
       "      <td>Hiburan</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972</th>\n",
       "      <td>93184085</td>\n",
       "      <td>Otomotif</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015</th>\n",
       "      <td>93195291</td>\n",
       "      <td>Hiburan</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2250</th>\n",
       "      <td>93201544</td>\n",
       "      <td>Hiburan</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3276</th>\n",
       "      <td>93213891</td>\n",
       "      <td>Teknologi</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4150</th>\n",
       "      <td>93197166</td>\n",
       "      <td>Hiburan</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4338</th>\n",
       "      <td>93186717</td>\n",
       "      <td>Sepak Bola</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4750</th>\n",
       "      <td>93224988</td>\n",
       "      <td>Sepak Bola</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4838</th>\n",
       "      <td>93183169</td>\n",
       "      <td>Politik</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4917</th>\n",
       "      <td>93194456</td>\n",
       "      <td>Hiburan</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5126</th>\n",
       "      <td>93184755</td>\n",
       "      <td>Hiburan</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5306</th>\n",
       "      <td>93195757</td>\n",
       "      <td>Lifestyle</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5859</th>\n",
       "      <td>93208751</td>\n",
       "      <td>Hiburan</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5965</th>\n",
       "      <td>93189109</td>\n",
       "      <td>Lifestyle</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6032</th>\n",
       "      <td>93201441</td>\n",
       "      <td>Internasional</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6094</th>\n",
       "      <td>93187589</td>\n",
       "      <td>Hiburan</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6442</th>\n",
       "      <td>947838</td>\n",
       "      <td>Ekonomi</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6539</th>\n",
       "      <td>93181264</td>\n",
       "      <td>Lifestyle</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6689</th>\n",
       "      <td>845044</td>\n",
       "      <td>Hiburan</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6760</th>\n",
       "      <td>93188937</td>\n",
       "      <td>Hiburan</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6816</th>\n",
       "      <td>93195181</td>\n",
       "      <td>Internasional</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6886</th>\n",
       "      <td>93201878</td>\n",
       "      <td>Sepak Bola</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7239</th>\n",
       "      <td>93187815</td>\n",
       "      <td>Hiburan</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7307</th>\n",
       "      <td>93182251</td>\n",
       "      <td>Hiburan</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7403</th>\n",
       "      <td>93186538</td>\n",
       "      <td>Hiburan</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7425</th>\n",
       "      <td>93186516</td>\n",
       "      <td>Sepak Bola</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7490</th>\n",
       "      <td>93192172</td>\n",
       "      <td>Teknologi</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7526</th>\n",
       "      <td>93203404</td>\n",
       "      <td>Hiburan</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7920</th>\n",
       "      <td>93208905</td>\n",
       "      <td>Hiburan</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8457</th>\n",
       "      <td>93201720</td>\n",
       "      <td>Teknologi</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8833</th>\n",
       "      <td>93189224</td>\n",
       "      <td>Ekonomi</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8851</th>\n",
       "      <td>93191464</td>\n",
       "      <td>Lifestyle</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9552</th>\n",
       "      <td>93183963</td>\n",
       "      <td>Hiburan</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9696</th>\n",
       "      <td>93201912</td>\n",
       "      <td>Hiburan</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      article_id  article_topic article_content\n",
       "197     93210288      Teknologi             NaN\n",
       "674     93185319        Hiburan             NaN\n",
       "817     93189481        Hiburan             NaN\n",
       "972     93184085       Otomotif             NaN\n",
       "2015    93195291        Hiburan             NaN\n",
       "2250    93201544        Hiburan             NaN\n",
       "3276    93213891      Teknologi             NaN\n",
       "4150    93197166        Hiburan             NaN\n",
       "4338    93186717     Sepak Bola             NaN\n",
       "4750    93224988     Sepak Bola             NaN\n",
       "4838    93183169        Politik             NaN\n",
       "4917    93194456        Hiburan             NaN\n",
       "5126    93184755        Hiburan             NaN\n",
       "5306    93195757      Lifestyle             NaN\n",
       "5859    93208751        Hiburan             NaN\n",
       "5965    93189109      Lifestyle             NaN\n",
       "6032    93201441  Internasional             NaN\n",
       "6094    93187589        Hiburan             NaN\n",
       "6442      947838        Ekonomi             NaN\n",
       "6539    93181264      Lifestyle             NaN\n",
       "6689      845044        Hiburan             NaN\n",
       "6760    93188937        Hiburan             NaN\n",
       "6816    93195181  Internasional             NaN\n",
       "6886    93201878     Sepak Bola             NaN\n",
       "7239    93187815        Hiburan             NaN\n",
       "7307    93182251        Hiburan             NaN\n",
       "7403    93186538        Hiburan             NaN\n",
       "7425    93186516     Sepak Bola             NaN\n",
       "7490    93192172      Teknologi             NaN\n",
       "7526    93203404        Hiburan             NaN\n",
       "7920    93208905        Hiburan             NaN\n",
       "8457    93201720      Teknologi             NaN\n",
       "8833    93189224        Ekonomi             NaN\n",
       "8851    93191464      Lifestyle             NaN\n",
       "9552    93183963        Hiburan             NaN\n",
       "9696    93201912        Hiburan             NaN"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## check any cells that have missing value\n",
    "data_csv[data_csv['article_content'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "article_id         0\n",
       "article_topic      0\n",
       "article_content    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## drop those cells\n",
    "data_csv=data_csv.dropna(how='any')\n",
    "data_csv.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of topics:\n",
      "Internasional, Ekonomi, Teknologi, Hiburan, Haji, Travel, Personal, Sepak Bola, Health, Sports, Politik, Otomotif, KPK, Lifestyle, Keuangan, Sejarah, Regional, Pendidikan, Hukum, Obat-obatan, Bojonegoro, Kesehatan, Horor, Bisnis, MotoGP, Sains, Jakarta, Pilgub Jatim, K-Pop\n"
     ]
    }
   ],
   "source": [
    "## find unique values from 'article_topic'\n",
    "topi=data_csv['article_topic'].unique().tolist()\n",
    "print('List of topics:')\n",
    "print(*topi, sep=', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of Teknologi   567\n",
      "number of Sports   435\n",
      "number of Hukum   85\n",
      "number of Otomotif   173\n",
      "number of MotoGP   35\n",
      "number of Sains   174\n",
      "number of Sejarah   70\n",
      "number of Internasional   739\n",
      "number of Horor   50\n",
      "number of Bisnis   25\n",
      "number of Personal   81\n",
      "number of Politik   103\n",
      "number of Health   131\n",
      "number of Kesehatan   195\n",
      "number of Keuangan   14\n",
      "number of Hiburan   1448\n",
      "number of Regional   35\n",
      "number of Haji   1497\n",
      "number of Pilgub Jatim   25\n",
      "number of Travel   76\n",
      "number of Sepak Bola   1180\n",
      "number of Jakarta   12\n",
      "number of Lifestyle   568\n",
      "number of Obat-obatan   58\n",
      "number of Pendidikan   70\n",
      "number of Bojonegoro   260\n",
      "number of K-Pop   61\n",
      "number of KPK   37\n",
      "number of Ekonomi   1760\n"
     ]
    }
   ],
   "source": [
    "## class distribution\n",
    "for p in list(set(data_csv.article_topic)):\n",
    "    print('number of',p,' ',len(data_csv.loc[data_csv['article_topic'] == p]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>article_topic</th>\n",
       "      <th>article_content</th>\n",
       "      <th>index_topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>93205794</td>\n",
       "      <td>Internasional</td>\n",
       "      <td>Kepolisian Inggris tengah memburu pelaku yang...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>93186698</td>\n",
       "      <td>Ekonomi</td>\n",
       "      <td>Seluruh layanan transaksi di jalan tol akan m...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>93191463</td>\n",
       "      <td>Teknologi</td>\n",
       "      <td>\\nHari ini, Rabu (23/8), ternyata menjadi har...</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>93219292</td>\n",
       "      <td>Ekonomi</td>\n",
       "      <td>Saat ini Indonesia hanya memiliki cadangan ba...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>343106</td>\n",
       "      <td>Hiburan</td>\n",
       "      <td>Hari ini, Selasa (1/8), pedangdut Ridho Rhoma...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id  article_topic  \\\n",
       "0    93205794  Internasional   \n",
       "1    93186698        Ekonomi   \n",
       "2    93191463      Teknologi   \n",
       "3    93219292        Ekonomi   \n",
       "4      343106        Hiburan   \n",
       "\n",
       "                                     article_content  index_topic  \n",
       "0   Kepolisian Inggris tengah memburu pelaku yang...            8  \n",
       "1   Seluruh layanan transaksi di jalan tol akan m...            2  \n",
       "2   \\nHari ini, Rabu (23/8), ternyata menjadi har...           27  \n",
       "3   Saat ini Indonesia hanya memiliki cadangan ba...            2  \n",
       "4   Hari ini, Selasa (1/8), pedangdut Ridho Rhoma...            5  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## convert categorical values into numeric values\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(data_csv['article_topic'])\n",
    "data_csv['index_topic']= encoder.transform(data_csv['article_topic'])\n",
    "data_csv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "raw text of training and test data will be transformed into new feature by going through some processes below. Stopword removing is to remove the most common words in a language and stemming is to reduce a word by remove its affixes. After re-runing some process multiple times, we noticed there were rows that had the same article contents. Therefore, the latest version of the script for stemming and stopword removing was created as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import StemmerFactory class\n",
    "factory = StopWordRemoverFactory()\n",
    "stopword = factory.create_stop_word_remover()\n",
    "\n",
    "## create stemmer\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "## create new column of stemmed and stopwords removed articles\n",
    "a_content=data_csv.article_content\n",
    "\n",
    "for i in a_content:\n",
    "    stops=stopword.remove(stemmer.stem(i))\n",
    "    wordy=''\n",
    "    for st in stops.split(' '):\n",
    "        if st.isalpha():      \n",
    "            wordy+=st+' '\n",
    "    # dealing with same articles\n",
    "    indx=data_csv.loc[data_csv['article_content']==i].index.tolist()\n",
    "    data_csv.loc[indx,'article_new']=wordy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After multiple process of trial and error, I found that there were a few duplicate articles, eg below shows that row 791 and 107 were the same. That's why i put this command in cell above\n",
    "``` pyhton\n",
    "    indx=data_csv.loc[data_csv['article_content']==i].index.tolist()\n",
    "    data_csv.loc[indx,'article_new']=wordy\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>article_topic</th>\n",
       "      <th>article_content</th>\n",
       "      <th>index_topic</th>\n",
       "      <th>article_new</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>1599799</td>\n",
       "      <td>Haji</td>\n",
       "      <td>KBRN, Madiun (MCH) : Kepala kantor Kementeria...</td>\n",
       "      <td>3</td>\n",
       "      <td>kbrn madiun mch kepala kantor menteri agama ke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>791</th>\n",
       "      <td>93181816</td>\n",
       "      <td>Haji</td>\n",
       "      <td>KBRN, Madiun (MCH) : Kepala kantor Kementeria...</td>\n",
       "      <td>3</td>\n",
       "      <td>kbrn madiun mch kepala kantor menteri agama ke...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     article_id article_topic  \\\n",
       "107     1599799          Haji   \n",
       "791    93181816          Haji   \n",
       "\n",
       "                                       article_content  index_topic  \\\n",
       "107   KBRN, Madiun (MCH) : Kepala kantor Kementeria...            3   \n",
       "791   KBRN, Madiun (MCH) : Kepala kantor Kementeria...            3   \n",
       "\n",
       "                                           article_new  \n",
       "107  kbrn madiun mch kepala kantor menteri agama ke...  \n",
       "791  kbrn madiun mch kepala kantor menteri agama ke...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_csv.loc[data_csv['article_content']==data_csv['article_content'][791]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_csv.loc[data_csv['article_content']==data_csv['article_content'][791]].index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "791"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_csv.loc[data_csv['article_content']==data_csv['article_content'][791]].index[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset:  8469\n",
      "Test dataset:  1495\n"
     ]
    }
   ],
   "source": [
    "## splitting\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_csv['article_new'], data_csv['index_topic'], test_size=.15, random_state = 79)\n",
    "print(\"Training dataset: \", X_train.shape[0])\n",
    "print(\"Test dataset: \", X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "##  instantiate CountVectorizer()\n",
    "cv = CountVectorizer()\n",
    " \n",
    "## this steps generates word counts for the words in your dataset\n",
    "x_word_count_vector = cv.fit_transform(X_train)\n",
    "```\n",
    "#### After splitting, we ran the cell above and we got an error message: \n",
    "```python\n",
    "---------------------------------------------------------------------------\n",
    "AttributeError                            Traceback (most recent call last)\n",
    "<ipython-input-13-6036975bf677> in <module>\n",
    "      3 \n",
    "      4 ## this steps generates word counts for the words in your dataset\n",
    "----> 5 x_word_count_vector = cv.fit_transform(X_train)\n",
    "      6 ## word_count_vector\n",
    "\n",
    "~/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py in fit_transform(self, raw_documents, y)\n",
    "   1029 \n",
    "   1030         vocabulary, X = self._count_vocab(raw_documents,\n",
    "-> 1031                                           self.fixed_vocabulary_)\n",
    "   1032 \n",
    "   1033         if self.binary:\n",
    "\n",
    "~/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py in _count_vocab(self, raw_documents, fixed_vocab)\n",
    "    941         for doc in raw_documents:\n",
    "    942             feature_counter = {}\n",
    "--> 943             for feature in analyze(doc):\n",
    "    944                 try:\n",
    "    945                     feature_idx = vocabulary[feature]\n",
    "\n",
    "~/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py in <lambda>(doc)\n",
    "    327                                                tokenize)\n",
    "    328             return lambda doc: self._word_ngrams(\n",
    "--> 329                 tokenize(preprocess(self.decode(doc))), stop_words)\n",
    "    330 \n",
    "    331         else:\n",
    "\n",
    "~/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py in <lambda>(x)\n",
    "    255 \n",
    "    256         if self.lowercase:\n",
    "--> 257             return lambda x: strip_accents(x.lower())\n",
    "    258         else:\n",
    "    259             return strip_accents\n",
    "\n",
    "AttributeError: 'float' object has no attribute 'lower'\n",
    "```\n",
    "#### it turned out there were missing values in some rows, we would analize how it could be possible. We already removed the missing value though. Perhaps, we would track down where the problem was"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>article_topic</th>\n",
       "      <th>article_content</th>\n",
       "      <th>index_topic</th>\n",
       "      <th>article_new</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>93181418</td>\n",
       "      <td>Hiburan</td>\n",
       "      <td></td>\n",
       "      <td>5</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1331</th>\n",
       "      <td>93191657</td>\n",
       "      <td>Hiburan</td>\n",
       "      <td>,</td>\n",
       "      <td>5</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1432</th>\n",
       "      <td>93212920</td>\n",
       "      <td>Kesehatan</td>\n",
       "      <td></td>\n",
       "      <td>12</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1601</th>\n",
       "      <td>93181411</td>\n",
       "      <td>Hiburan</td>\n",
       "      <td></td>\n",
       "      <td>5</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2031</th>\n",
       "      <td>93210296</td>\n",
       "      <td>Health</td>\n",
       "      <td>.,</td>\n",
       "      <td>4</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2342</th>\n",
       "      <td>93190978</td>\n",
       "      <td>Hiburan</td>\n",
       "      <td></td>\n",
       "      <td>5</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2937</th>\n",
       "      <td>93190761</td>\n",
       "      <td>Hiburan</td>\n",
       "      <td>.</td>\n",
       "      <td>5</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3600</th>\n",
       "      <td>1485824</td>\n",
       "      <td>Hiburan</td>\n",
       "      <td></td>\n",
       "      <td>5</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3730</th>\n",
       "      <td>93191661</td>\n",
       "      <td>Hiburan</td>\n",
       "      <td>,</td>\n",
       "      <td>5</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4085</th>\n",
       "      <td>1586004</td>\n",
       "      <td>Politik</td>\n",
       "      <td></td>\n",
       "      <td>21</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4087</th>\n",
       "      <td>4052810</td>\n",
       "      <td>Politik</td>\n",
       "      <td></td>\n",
       "      <td>21</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4173</th>\n",
       "      <td>93191699</td>\n",
       "      <td>Hiburan</td>\n",
       "      <td>.</td>\n",
       "      <td>5</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5297</th>\n",
       "      <td>93216759</td>\n",
       "      <td>Kesehatan</td>\n",
       "      <td></td>\n",
       "      <td>12</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6352</th>\n",
       "      <td>1484488</td>\n",
       "      <td>Sepak Bola</td>\n",
       "      <td></td>\n",
       "      <td>25</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6887</th>\n",
       "      <td>93181415</td>\n",
       "      <td>Hiburan</td>\n",
       "      <td></td>\n",
       "      <td>5</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6942</th>\n",
       "      <td>93188749</td>\n",
       "      <td>Politik</td>\n",
       "      <td></td>\n",
       "      <td>21</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8074</th>\n",
       "      <td>93190979</td>\n",
       "      <td>Hiburan</td>\n",
       "      <td></td>\n",
       "      <td>5</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8697</th>\n",
       "      <td>93195225</td>\n",
       "      <td>Kesehatan</td>\n",
       "      <td></td>\n",
       "      <td>12</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9261</th>\n",
       "      <td>93191674</td>\n",
       "      <td>Hiburan</td>\n",
       "      <td>.</td>\n",
       "      <td>5</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9899</th>\n",
       "      <td>93228169</td>\n",
       "      <td>Personal</td>\n",
       "      <td>Hello</td>\n",
       "      <td>19</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      article_id article_topic article_content  index_topic article_new\n",
       "555     93181418       Hiburan                            5            \n",
       "1331    93191657       Hiburan               ,            5            \n",
       "1432    93212920     Kesehatan                           12            \n",
       "1601    93181411       Hiburan                            5            \n",
       "2031    93210296        Health            .,              4            \n",
       "2342    93190978       Hiburan                            5            \n",
       "2937    93190761       Hiburan               .            5            \n",
       "3600     1485824       Hiburan                            5            \n",
       "3730    93191661       Hiburan               ,            5            \n",
       "4085     1586004       Politik                           21            \n",
       "4087     4052810       Politik                           21            \n",
       "4173    93191699       Hiburan               .            5            \n",
       "5297    93216759     Kesehatan                           12            \n",
       "6352     1484488    Sepak Bola                           25            \n",
       "6887    93181415       Hiburan                            5            \n",
       "6942    93188749       Politik                           21            \n",
       "8074    93190979       Hiburan                            5            \n",
       "8697    93195225     Kesehatan                           12            \n",
       "9261    93191674       Hiburan               .            5            \n",
       "9899    93228169      Personal          Hello            19            "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## check if there's any string cell contains blank space\n",
    "id_=data_csv[data_csv['article_new']==''].index.tolist()\n",
    "data_csv[data_csv['article_new']=='']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## replace blank space with np.nan\n",
    "for i in id_:\n",
    "    data_csv.loc[i] = data_csv.loc[i].replace('',np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>article_topic</th>\n",
       "      <th>article_content</th>\n",
       "      <th>index_topic</th>\n",
       "      <th>article_new</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>93181418</td>\n",
       "      <td>Hiburan</td>\n",
       "      <td></td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1331</th>\n",
       "      <td>93191657</td>\n",
       "      <td>Hiburan</td>\n",
       "      <td>,</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1432</th>\n",
       "      <td>93212920</td>\n",
       "      <td>Kesehatan</td>\n",
       "      <td></td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1601</th>\n",
       "      <td>93181411</td>\n",
       "      <td>Hiburan</td>\n",
       "      <td></td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2031</th>\n",
       "      <td>93210296</td>\n",
       "      <td>Health</td>\n",
       "      <td>.,</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2342</th>\n",
       "      <td>93190978</td>\n",
       "      <td>Hiburan</td>\n",
       "      <td></td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2937</th>\n",
       "      <td>93190761</td>\n",
       "      <td>Hiburan</td>\n",
       "      <td>.</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3600</th>\n",
       "      <td>1485824</td>\n",
       "      <td>Hiburan</td>\n",
       "      <td></td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3730</th>\n",
       "      <td>93191661</td>\n",
       "      <td>Hiburan</td>\n",
       "      <td>,</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4085</th>\n",
       "      <td>1586004</td>\n",
       "      <td>Politik</td>\n",
       "      <td></td>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4087</th>\n",
       "      <td>4052810</td>\n",
       "      <td>Politik</td>\n",
       "      <td></td>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4173</th>\n",
       "      <td>93191699</td>\n",
       "      <td>Hiburan</td>\n",
       "      <td>.</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5297</th>\n",
       "      <td>93216759</td>\n",
       "      <td>Kesehatan</td>\n",
       "      <td></td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6352</th>\n",
       "      <td>1484488</td>\n",
       "      <td>Sepak Bola</td>\n",
       "      <td></td>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6887</th>\n",
       "      <td>93181415</td>\n",
       "      <td>Hiburan</td>\n",
       "      <td></td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6942</th>\n",
       "      <td>93188749</td>\n",
       "      <td>Politik</td>\n",
       "      <td></td>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8074</th>\n",
       "      <td>93190979</td>\n",
       "      <td>Hiburan</td>\n",
       "      <td></td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8697</th>\n",
       "      <td>93195225</td>\n",
       "      <td>Kesehatan</td>\n",
       "      <td></td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9261</th>\n",
       "      <td>93191674</td>\n",
       "      <td>Hiburan</td>\n",
       "      <td>.</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9899</th>\n",
       "      <td>93228169</td>\n",
       "      <td>Personal</td>\n",
       "      <td>Hello</td>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      article_id article_topic article_content  index_topic article_new\n",
       "555     93181418       Hiburan                            5         NaN\n",
       "1331    93191657       Hiburan               ,            5         NaN\n",
       "1432    93212920     Kesehatan                           12         NaN\n",
       "1601    93181411       Hiburan                            5         NaN\n",
       "2031    93210296        Health            .,              4         NaN\n",
       "2342    93190978       Hiburan                            5         NaN\n",
       "2937    93190761       Hiburan               .            5         NaN\n",
       "3600     1485824       Hiburan                            5         NaN\n",
       "3730    93191661       Hiburan               ,            5         NaN\n",
       "4085     1586004       Politik                           21         NaN\n",
       "4087     4052810       Politik                           21         NaN\n",
       "4173    93191699       Hiburan               .            5         NaN\n",
       "5297    93216759     Kesehatan                           12         NaN\n",
       "6352     1484488    Sepak Bola                           25         NaN\n",
       "6887    93181415       Hiburan                            5         NaN\n",
       "6942    93188749       Politik                           21         NaN\n",
       "8074    93190979       Hiburan                            5         NaN\n",
       "8697    93195225     Kesehatan                           12         NaN\n",
       "9261    93191674       Hiburan               .            5         NaN\n",
       "9899    93228169      Personal          Hello            19         NaN"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_csv[data_csv['article_new'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "article_id          0\n",
       "article_topic       0\n",
       "article_content     0\n",
       "index_topic         0\n",
       "article_new        20\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_csv.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There were rows whose 'article_content' only contained spaces '  '. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  .'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_csv.article_content[9261]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "article_id         0\n",
       "article_topic      0\n",
       "article_content    0\n",
       "index_topic        0\n",
       "article_new        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## drop those cells\n",
    "data_new=data_csv.dropna(how='any')\n",
    "data_new.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data to training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset:  8452\n",
      "Test dataset:  1492\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data_new['article_new'], data_new['index_topic'], test_size=.15, random_state = 89)\n",
    "print(\"Training dataset: \", X_train.shape[0])\n",
    "print(\"Test dataset: \", X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  instantiate CountVectorizer()\n",
    "cv = CountVectorizer()\n",
    " \n",
    "## this steps generates word counts for the words in your dataset\n",
    "x_word_count_vector = cv.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_testing_count = cv.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "na_bayes = MultinomialNB()\n",
    "na_bayes.fit(x_word_count_vector, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_nb = na_bayes.predict(x_testing_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/devi/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "       early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
       "       l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=None,\n",
       "       n_iter=None, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
       "       power_t=0.5, random_state=42, shuffle=True, tol=None,\n",
       "       validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgdc_=SGDClassifier(random_state=42)\n",
    "sgdc_.fit(x_word_count_vector, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_sgdc = sgdc_.predict(x_testing_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accurary of Naive Bayes: 0.8230563002680965\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       Bisnis       0.00      0.00      0.00         2\n",
      "   Bojonegoro       0.78      0.83      0.81        30\n",
      "      Ekonomi       0.87      0.97      0.92       280\n",
      "         Haji       0.98      0.98      0.98       253\n",
      "       Health       0.54      0.58      0.56        26\n",
      "      Hiburan       0.79      0.99      0.88       188\n",
      "        Horor       0.00      0.00      0.00         5\n",
      "        Hukum       0.67      0.50      0.57         8\n",
      "Internasional       0.78      0.87      0.82       107\n",
      "      Jakarta       0.00      0.00      0.00         3\n",
      "        K-Pop       1.00      0.22      0.36         9\n",
      "          KPK       1.00      1.00      1.00         3\n",
      "    Kesehatan       0.50      0.61      0.55        33\n",
      "     Keuangan       0.00      0.00      0.00         3\n",
      "    Lifestyle       0.67      0.78      0.72        92\n",
      "       MotoGP       0.00      0.00      0.00        10\n",
      "  Obat-obatan       0.00      0.00      0.00        13\n",
      "     Otomotif       1.00      0.67      0.80        18\n",
      "   Pendidikan       1.00      0.31      0.47        13\n",
      "     Personal       1.00      0.12      0.22         8\n",
      " Pilgub Jatim       0.00      0.00      0.00         5\n",
      "      Politik       0.44      0.29      0.35        14\n",
      "     Regional       0.00      0.00      0.00         2\n",
      "        Sains       0.00      0.00      0.00        27\n",
      "      Sejarah       0.00      0.00      0.00         8\n",
      "   Sepak Bola       0.83      0.99      0.90       172\n",
      "       Sports       0.62      0.42      0.50        62\n",
      "    Teknologi       0.90      0.78      0.84        88\n",
      "       Travel       0.00      0.00      0.00        10\n",
      "\n",
      "    micro avg       0.82      0.82      0.82      1492\n",
      "    macro avg       0.50      0.41      0.42      1492\n",
      " weighted avg       0.78      0.82      0.79      1492\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/devi/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print('Accurary of Naive Bayes:', accuracy_score(y_test,pred_nb))\n",
    "print(classification_report(y_test, pred_nb, target_names=encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accurary of SGDC: 0.8512064343163539\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       Bisnis       0.50      1.00      0.67         2\n",
      "   Bojonegoro       0.85      0.93      0.89        30\n",
      "      Ekonomi       0.95      0.94      0.95       280\n",
      "         Haji       0.97      1.00      0.98       253\n",
      "       Health       0.38      0.35      0.36        26\n",
      "      Hiburan       0.91      0.97      0.94       188\n",
      "        Horor       1.00      0.60      0.75         5\n",
      "        Hukum       0.55      0.75      0.63         8\n",
      "Internasional       0.82      0.82      0.82       107\n",
      "      Jakarta       0.00      0.00      0.00         3\n",
      "        K-Pop       0.56      0.56      0.56         9\n",
      "          KPK       1.00      1.00      1.00         3\n",
      "    Kesehatan       0.40      0.52      0.45        33\n",
      "     Keuangan       1.00      0.67      0.80         3\n",
      "    Lifestyle       0.84      0.70      0.76        92\n",
      "       MotoGP       0.73      0.80      0.76        10\n",
      "  Obat-obatan       0.38      0.38      0.38        13\n",
      "     Otomotif       1.00      0.83      0.91        18\n",
      "   Pendidikan       0.73      0.62      0.67        13\n",
      "     Personal       0.43      0.38      0.40         8\n",
      " Pilgub Jatim       0.31      0.80      0.44         5\n",
      "      Politik       1.00      0.36      0.53        14\n",
      "     Regional       0.00      0.00      0.00         2\n",
      "        Sains       0.76      0.81      0.79        27\n",
      "      Sejarah       0.33      0.12      0.18         8\n",
      "   Sepak Bola       0.82      0.96      0.89       172\n",
      "       Sports       0.74      0.42      0.54        62\n",
      "    Teknologi       0.95      0.86      0.90        88\n",
      "       Travel       0.67      0.60      0.63        10\n",
      "\n",
      "    micro avg       0.85      0.85      0.85      1492\n",
      "    macro avg       0.67      0.65      0.64      1492\n",
      " weighted avg       0.86      0.85      0.85      1492\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Accurary of SGDC:', accuracy_score(y_test,pred_sgdc))\n",
    "print(classification_report(y_test, pred_sgdc, target_names=encoder.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing SGDClassifier to MultinomialNB, the accuracy of the SGDClassifier model was higher by 0.028, therefore I chosed the SGDClassifier to perform the next step which was hypertuning\n",
    "# Hypertuning\n",
    "using RandomSearch to find the best combination of parameters for building the model by randomly selecting a set of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create regularization penalty space\n",
    "penalty = ['l1', 'l2']\n",
    "\n",
    "# Create regularization max_iter\n",
    "max_iter = [5, 100, 1000] \n",
    "\n",
    "# Create regularization alpha\n",
    "alpha = [1e-3, 1e-4, 1e-5] \n",
    "\n",
    "# Create regularization tol\n",
    "tol = [1e-3, None, 1e-5] \n",
    "\n",
    "# Create hyperparameter options\n",
    "hyperparameters = dict(alpha=alpha, penalty=penalty, max_iter=max_iter, tol=tol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/devi/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "mo_ran = RandomizedSearchCV(sgdc_, hyperparameters, random_state=1, n_iter=10, cv=5, verbose=0, n_jobs=-1)\n",
    "# Fit randomized search\n",
    "ran_fit = mo_ran.fit(x_word_count_vector, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tol': None, 'penalty': 'l2', 'max_iter': 1000, 'alpha': 0.0001}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ran_fit.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ran = ran_fit.predict(x_testing_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accurary of SGDClass+RandomSearch (optimum parameter): 0.8552278820375335\n"
     ]
    }
   ],
   "source": [
    "print('Accurary of SGDClass+RandomSearch (optimum parameter):', accuracy_score(y_test,pred_ran))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "       Bisnis       0.33      0.50      0.40         2\n",
      "   Bojonegoro       0.93      0.90      0.92        30\n",
      "      Ekonomi       0.93      0.97      0.95       280\n",
      "         Haji       0.98      0.99      0.99       253\n",
      "       Health       0.29      0.31      0.30        26\n",
      "      Hiburan       0.93      0.96      0.95       188\n",
      "        Horor       0.80      0.80      0.80         5\n",
      "        Hukum       0.46      0.75      0.57         8\n",
      "Internasional       0.89      0.86      0.88       107\n",
      "      Jakarta       0.00      0.00      0.00         3\n",
      "        K-Pop       0.73      0.89      0.80         9\n",
      "          KPK       0.75      1.00      0.86         3\n",
      "    Kesehatan       0.39      0.45      0.42        33\n",
      "     Keuangan       1.00      0.33      0.50         3\n",
      "    Lifestyle       0.83      0.76      0.80        92\n",
      "       MotoGP       1.00      0.70      0.82        10\n",
      "  Obat-obatan       0.33      0.15      0.21        13\n",
      "     Otomotif       0.82      0.78      0.80        18\n",
      "   Pendidikan       0.88      0.54      0.67        13\n",
      "     Personal       0.38      0.38      0.38         8\n",
      " Pilgub Jatim       0.57      0.80      0.67         5\n",
      "      Politik       0.54      0.50      0.52        14\n",
      "     Regional       0.00      0.00      0.00         2\n",
      "        Sains       0.76      0.96      0.85        27\n",
      "      Sejarah       0.60      0.38      0.46         8\n",
      "   Sepak Bola       0.82      0.90      0.86       172\n",
      "       Sports       0.64      0.48      0.55        62\n",
      "    Teknologi       0.94      0.85      0.89        88\n",
      "       Travel       0.75      0.60      0.67        10\n",
      "\n",
      "    micro avg       0.86      0.86      0.86      1492\n",
      "    macro avg       0.66      0.64      0.64      1492\n",
      " weighted avg       0.85      0.86      0.85      1492\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/devi/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, pred_ran, target_names=encoder.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With RandomSearch, we could improve accuracy of SGDClassifier by 0.004 and it's about 0.855\n",
    "### Create data frame to store the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_new=pd.DataFrame(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "inx=y_new.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in inx:\n",
    "    y_new.loc[i,'article_topic']=data_new.loc[i,'article_topic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_new['idx_pred']=pred_ran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_new['pred']=list(encoder.inverse_transform(pred_ran))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index_topic</th>\n",
       "      <th>article_topic</th>\n",
       "      <th>idx_pred</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6472</th>\n",
       "      <td>3</td>\n",
       "      <td>Haji</td>\n",
       "      <td>3</td>\n",
       "      <td>Haji</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5277</th>\n",
       "      <td>1</td>\n",
       "      <td>Bojonegoro</td>\n",
       "      <td>0</td>\n",
       "      <td>Bisnis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601</th>\n",
       "      <td>3</td>\n",
       "      <td>Haji</td>\n",
       "      <td>3</td>\n",
       "      <td>Haji</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5130</th>\n",
       "      <td>25</td>\n",
       "      <td>Sepak Bola</td>\n",
       "      <td>25</td>\n",
       "      <td>Sepak Bola</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9220</th>\n",
       "      <td>3</td>\n",
       "      <td>Haji</td>\n",
       "      <td>3</td>\n",
       "      <td>Haji</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6305</th>\n",
       "      <td>5</td>\n",
       "      <td>Hiburan</td>\n",
       "      <td>5</td>\n",
       "      <td>Hiburan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9157</th>\n",
       "      <td>26</td>\n",
       "      <td>Sports</td>\n",
       "      <td>26</td>\n",
       "      <td>Sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>872</th>\n",
       "      <td>2</td>\n",
       "      <td>Ekonomi</td>\n",
       "      <td>2</td>\n",
       "      <td>Ekonomi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2779</th>\n",
       "      <td>2</td>\n",
       "      <td>Ekonomi</td>\n",
       "      <td>2</td>\n",
       "      <td>Ekonomi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7135</th>\n",
       "      <td>25</td>\n",
       "      <td>Sepak Bola</td>\n",
       "      <td>25</td>\n",
       "      <td>Sepak Bola</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3969</th>\n",
       "      <td>12</td>\n",
       "      <td>Kesehatan</td>\n",
       "      <td>12</td>\n",
       "      <td>Kesehatan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5476</th>\n",
       "      <td>3</td>\n",
       "      <td>Haji</td>\n",
       "      <td>3</td>\n",
       "      <td>Haji</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9850</th>\n",
       "      <td>25</td>\n",
       "      <td>Sepak Bola</td>\n",
       "      <td>25</td>\n",
       "      <td>Sepak Bola</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>4</td>\n",
       "      <td>Health</td>\n",
       "      <td>12</td>\n",
       "      <td>Kesehatan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>955</th>\n",
       "      <td>20</td>\n",
       "      <td>Pilgub Jatim</td>\n",
       "      <td>21</td>\n",
       "      <td>Politik</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2177</th>\n",
       "      <td>25</td>\n",
       "      <td>Sepak Bola</td>\n",
       "      <td>25</td>\n",
       "      <td>Sepak Bola</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8491</th>\n",
       "      <td>2</td>\n",
       "      <td>Ekonomi</td>\n",
       "      <td>2</td>\n",
       "      <td>Ekonomi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3414</th>\n",
       "      <td>18</td>\n",
       "      <td>Pendidikan</td>\n",
       "      <td>23</td>\n",
       "      <td>Sains</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7672</th>\n",
       "      <td>2</td>\n",
       "      <td>Ekonomi</td>\n",
       "      <td>2</td>\n",
       "      <td>Ekonomi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1661</th>\n",
       "      <td>3</td>\n",
       "      <td>Haji</td>\n",
       "      <td>3</td>\n",
       "      <td>Haji</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      index_topic article_topic  idx_pred        pred\n",
       "6472            3          Haji         3        Haji\n",
       "5277            1    Bojonegoro         0      Bisnis\n",
       "601             3          Haji         3        Haji\n",
       "5130           25    Sepak Bola        25  Sepak Bola\n",
       "9220            3          Haji         3        Haji\n",
       "6305            5       Hiburan         5     Hiburan\n",
       "9157           26        Sports        26      Sports\n",
       "872             2       Ekonomi         2     Ekonomi\n",
       "2779            2       Ekonomi         2     Ekonomi\n",
       "7135           25    Sepak Bola        25  Sepak Bola\n",
       "3969           12     Kesehatan        12   Kesehatan\n",
       "5476            3          Haji         3        Haji\n",
       "9850           25    Sepak Bola        25  Sepak Bola\n",
       "95              4        Health        12   Kesehatan\n",
       "955            20  Pilgub Jatim        21     Politik\n",
       "2177           25    Sepak Bola        25  Sepak Bola\n",
       "8491            2       Ekonomi         2     Ekonomi\n",
       "3414           18    Pendidikan        23       Sains\n",
       "7672            2       Ekonomi         2     Ekonomi\n",
       "1661            3          Haji         3        Haji"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_new.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display top 10 words that occured most frequently in each topic's articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "       early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
       "       l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=1000,\n",
       "       n_iter=None, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
       "       power_t=0.5, random_state=42, shuffle=True, tol=None,\n",
       "       validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## model with optimum parameters\n",
    "sgdc_b=SGDClassifier(random_state=42,penalty='l2', tol=None, max_iter=1000, alpha=0.0001)\n",
    "sgdc_b.fit(x_word_count_vector, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_vo = {}\n",
    "vocab = cv.vocabulary_\n",
    "for word in vocab:\n",
    "    index = vocab[word]\n",
    "    reverse_vo[index] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list of most frequent words :\n",
      "Bisnis - ['steak', 'beliau', 'rintis', 'tapcash', 'slack', 'bisnis', 'lucy', 'buka', 'era', 'sapi'] \n",
      "\n",
      "Bojonegoro - ['lis', 'putar', 'berita', 'mu', 'html', 'link', 'com', 'reporter', 'blokbojonegoro', 'bojonegoro'] \n",
      "\n",
      "Ekonomi - ['kumpar', 'achsien', 'bendung', 'kutip', 'yg', 'persero', 'bank', 'esdm', 'selasa', 'menteri'] \n",
      "\n",
      "Haji - ['kemenag', 'jamaah', 'id', 'co', 'https', 'read', 'sumber', 'haji', 'mch', 'tes'] \n",
      "\n",
      "Health - ['haid', 'herbal', 'efek', 'koreng', 'ubah', 'nasofaring', 'risiko', 'nyamuk', 'nyaman', 'alas'] \n",
      "\n",
      "Hiburan - ['ratnacece', 'suara', 'nih', 'kalo', 'nyanyi', 'teamsusahmoveon', 'hahahaha', 'team', 'sih', 'lucu'] \n",
      "\n",
      "Horor - ['ekor', 'mistis', 'lendra', 'wujud', 'jelma', 'aktifitas', 'frasa', 'tasyakkul', 'jinn', 'dimensi'] \n",
      "\n",
      "Hukum - ['mahkamah', 'febri', 'daring', 'korupsi', 'sangka', 'narapidana', 'novel', 'periksa', 'novanto', 'bener'] \n",
      "\n",
      "Internasional - ['kamis', 'rabu', 'kiamat', 'tewas', 'polisi', 'lansir', 'press', 'reuters', 'associated', 'kutip'] \n",
      "\n",
      "Jakarta - ['publik', 'run', 'but', 'jakarta', 'semper', 'that', 'limbah', 'you', 'peta', 'lurah'] \n",
      "\n",
      "K-Pop - ['jung', 'chingudeul', 'debut', 'group', 'tanggal', 'anggota', 'ulang', 'mv', 'rilis', 'comeback'] \n",
      "\n",
      "KPK - ['ta', 'tanda', 'juara', 'duduk', 'lumpur', 'malang', 'koruptor', 'topeng', 'tpk', 'segel'] \n",
      "\n",
      "Kesehatan - ['dok', 'jelly', 'bengkak', 'sendi', 'organ', 'radang', 'lutut', 'tuntas', 'efeksamping', 'tubuhm'] \n",
      "\n",
      "Keuangan - ['uang', 'bijak', 'ilegal', 'sistem', 'management', 'manfaat', 'milik', 'investasi', 'pemprov', 'dki'] \n",
      "\n",
      "Lifestyle - ['cantik', 'us', 'kumpar', 'dr', 'percaya', 'pikir', 'santap', 'makeup', 'sejati', 'lansir'] \n",
      "\n",
      "MotoGP - ['cepat', 'lambat', 'petrucci', 'motogp', 'motor', 'misano', 'pedrosa', 'vinales', 'sirkuit', 'crash'] \n",
      "\n",
      "Obat-obatan - ['miom', 'kandung', 'formulasi', 'wasir', 'anus', 'komedo', 'hati', 'batuk', 'endometrium', 'tubuh'] \n",
      "\n",
      "Otomotif - ['cewek', 'lomba', 'sari', 'cowok', 'debu', 'halang', 'terik', 'pekat', 'gue', 'gaya'] \n",
      "\n",
      "Pendidikan - ['cabut', 'bincang', 'sunat', 'hehe', 'takut', 'hoax', 'nisan', 'placophobia', 'hapus', 'sayang'] \n",
      "\n",
      "Personal - ['kitamenjadi', 'iramameniti', 'kamusaat', 'tapak', 'lupa', 'suara', 'bait', 'tuh', 'hehehhe', 'hellojikaahshhshdhdjjdshhdjdjjdjdjjd'] \n",
      "\n",
      "Pilgub Jatim - ['rendra', 'calon', 'bangsaonline', 'gubernur', 'timur', 'pdip', 'bu', 'khofifah', 'risma', 'jatim'] \n",
      "\n",
      "Politik - ['ubk', 'pilkada', 'daerah', 'politik', 'probolinggo', 'tolong', 'tau', 'sholat', 'inspiring', 'quotes'] \n",
      "\n",
      "Regional - ['ansor', 'gresik', 'pemkot', 'trilyun', 'probolinggo', 'dinas', 'mojokerto', 'bangsaonline', 'pgri', 'pacitan'] \n",
      "\n",
      "Sains - ['kode', 'tutup', 'distraksi', 'serangga', 'indra', 'cahaya', 'robot', 'umami', 'sumber', 'gambar'] \n",
      "\n",
      "Sejarah - ['pandu', 'oktoberfest', 'gera', 'moguel', 'nama', 'tan', 'kolonial', 'blambangan', 'belanda', 'sejarah'] \n",
      "\n",
      "Sepak Bola - ['adoring', 'fandom', 'mimpi', 'olahraga', 'konyol', 'selamat', 'informasi', 'pusat', 'test', 'sporttoday'] \n",
      "\n",
      "Sports - ['duka', 'matchday', 'carvajal', 'swedia', 'allegri', 'vettel', 'detik', 'atlet', 'chamberlain', 'gp'] \n",
      "\n",
      "Teknologi - ['jack', 'warganet', 'milik', 'situs', 'twitter', 'karakter', 'fitur', 'tweet', 'hak', 'cipta'] \n",
      "\n",
      "Travel - ['libur', 'gaya', 'banyuwangi', 'festival', 'indah', 'lombok', 'getaway', 'traveling', 'belanak', 'selong'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "coefs = sgdc_b.coef_\n",
    "target_names = encoder.classes_\n",
    "print('list of most frequent words :')\n",
    "for i in range(len(target_names)):\n",
    "    words = []\n",
    "    for j in coefs[i].argsort()[-10:]:\n",
    "        words.append(reverse_vo[j])\n",
    "    print (target_names[i], '-', words, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict articles' topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp=data_new.article_content.loc[355]\n",
    "##to try different input, choose one below, please notice if you want to activate the line, disable the line above\n",
    "# inp={'article_content':['lucu sih teamsusahmoveon suara nih','rilis comeback mv jul jun','novanto bener periksa narapidana']}\n",
    "# inp={'article_content':[data_new.article_new.loc[355],data_new.article_new.loc[356]]}\n",
    "# inp=['lucu sih teamsusahmoveon suara nih','rilis comeback mv jul jun','novanto bener periksa narapidana']\n",
    "# inp='lucu sih teamsusahmoveon suara nih'\n",
    "\n",
    "\n",
    "if isinstance(inp, pd.DataFrame) is False:                \n",
    "    if type(inp)!=dict:\n",
    "        if type(inp)==str:\n",
    "            inp=[inp]\n",
    "        if len(inp)!=1:\n",
    "            inp=pd.DataFrame(inp)\n",
    "        else:\n",
    "            inp=pd.Series(inp)\n",
    "    else:\n",
    "        if len(inp.values())!=1:\n",
    "            inp=pd.DataFrame(inp.values())\n",
    "        else:\n",
    "            inp=list(inp.values())\n",
    "            inp=inp[0]\n",
    "            inp=pd.DataFrame(inp)\n",
    "\n",
    "# Data selection for dataframe and series\n",
    "if isinstance(inp, pd.DataFrame):\n",
    "    artic=inp.iloc[0,0]\n",
    "    d_artic=inp.iloc[:,0]\n",
    "else:\n",
    "    artic=inp.iloc[0]\n",
    "    d_artic=inp.values\n",
    "    \n",
    "# Stemming and stopword removing\n",
    "if artic not in data_new.article_new.values:\n",
    "    xy=[]\n",
    "    for i in d_artic:\n",
    "        stops_=stopword.remove(stemmer.stem(i))\n",
    "        wordy_=''\n",
    "        for st in stops_.split(' '):\n",
    "            if st.isalpha():      \n",
    "                wordy_+=st+' '\n",
    "        xy.append(wordy_)\n",
    "else:\n",
    "    xy=inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_t = cv.transform(xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['MotoGP']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('prediction:')\n",
    "list(encoder.inverse_transform(ran_fit.predict(x_t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true topic:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'MotoGP'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('true topic:')\n",
    "data_new['article_topic'].loc[355]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# - MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models for text classification I used were MultinomialNB and SGDClassifier because MultinomialNB works well with discrete features such as word counts and SGDClassifier works well with data represented as dense. For this experiment, SGDClassifier with no paramater used got a higher accuracy score, therefore I  did hyperparameter tuning to get a better SGDClassifier model by trying a pair of parameters and finding the optimal one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# - VALIDATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used accuracy because it indicated how good a model was to predict data correctly and I used classification report which also displayed f1 score, recall, prediction, and support (the actual number of occurrences of each class in data we predict) because it made us easier to understand the model perfomance and how good the model predicted some data dispersively. For example, in the classification report of MultinomialNB, it revealed that topic 'Horor' got 0 for f1 score, recall and prediction which meant that the model failed to predict all horor articles, whereas the number of horror articles (support) was 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# - FUTURE RESEARCH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming in Bahasa Indonesia is hard, because the data of possible affix combinations and root forms of words is found limited and we may face some problems, such as word sense ambiguity, for example, the word 'berikan' can be chopped as 'ber-i-kan' ('i' will be stored as the root form of word) or 'beri-kan' ('beri' will be extracted) or 'ber-ikan' ('ikan' will be returned instead) or for this case, 'belasan' will be stored as 'bas'. Another example, we can't identify name such as 'Aqilah' (it will be truncated as 'Aqil-ah' and 'Aqil' will be extracted) or in this case, 'Mekkah' as 'Mek'. So for the next research, we suppose to do correction manually for words that are falsely stated from the stemming process. Manually here means creating a function where we would define some root forms of words that may be inaccessible in sastrawi library and manually describe the root forms of the words instead. We can also try boosting method to improve accuracy score "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
